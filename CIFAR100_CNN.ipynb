{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "35e62453-6d56-4954-af7e-49ff9a941711",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  importing libraries\n",
    "import torch \n",
    "from torch import nn, load,save\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision.transforms import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.optim import Adam\n",
    "from torchvision.transforms import ToTensor\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "f182048f-b76d-4249-8d1e-45dd3f96191e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# setting up device configuration\n",
    "device=torch.device('cuda'if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "31c137d4-6658-4e52-9d1c-f1ff13ee6509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up hyperparameters\n",
    "epoch=20\n",
    "batchsize=32\n",
    "lr=1e-3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "7ec5005e-650f-424a-9596-569a11450629",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  transform dataset of range [0,1] to tensor of normalized range [-1,1]\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "8fb5ce01-73e9-41ee-841c-895f64204534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset=torchvision.datasets.CIFAR10(train=True,root='./dataset',download=True,transform=transform)\n",
    "test_dataset=torchvision.datasets.CIFAR10(train=False,root='./dataset',download=True,transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "ce7ff4de-d271-4076-b1e2-9a36329a7ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample shape: torch.Size([3, 32, 32])\n",
      "Label: 6\n"
     ]
    }
   ],
   "source": [
    "# Inspect the shape of the first sample\n",
    "sample, label = train_dataset[0]\n",
    "print(f'Sample shape: {sample.shape}')  # Should print torch.Size([3, 32, 32])\n",
    "print(f'Label: {label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "710e557b-dcab-41d4-a819-57bbc523e13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader=torch.utils.data.DataLoader(train_dataset, batch_size=batchsize,shuffle=True)\n",
    "test_dataloader=torch.utils.data.DataLoader(test_dataset,batch_size=batchsize, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "623d2ce2-5a18-4e3a-b86b-fa297c06a9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define classes\n",
    "classes=('plane','car','bird','cat','deer', 'dog', 'frog','horse', 'ship','truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "c04cb194-3389-4b12-99fb-dba6233bd216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional Network class\n",
    "class ImageClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model=nn.Sequential(\n",
    "            nn.Conv2d(3,32,(3,3)),  #3 x 32 X 32 -> 32-3+1 -> 32 x 30 X 30\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32,64,(3,3)), #32 x 30 X 30 -> 30-3+1 -> 64 x 28 X 28\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64,128,(3,3)), #64 x 28 X 28 -> 28-3+1 -> 128 x 26 x 26\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128*26* 26,10)\n",
    "            \n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.model(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e57e7ff8-dffd-44f3-a646-a00a34944ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  create an insitance of NN, loss and optimizer\n",
    "clf=ImageClassifier().to(device)\n",
    "opt=Adam(clf.parameters(),lr=lr)\n",
    "loss_func=nn.CrossEntropyLoss() #for multi-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "77d6e3b5-cdc3-46a5-aa1d-f4b62ee92503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, loss is : 0.02110102027654648 \n",
      "Epoch:1, loss is : 0.0002584050816949457 \n",
      "Epoch:2, loss is : 0.0008107690955512226 \n",
      "Epoch:3, loss is : 0.057763081043958664 \n",
      "Epoch:4, loss is : 0.004789986182004213 \n",
      "Epoch:5, loss is : 0.00045358837815001607 \n",
      "Epoch:6, loss is : 3.650685130196507e-06 \n",
      "Epoch:7, loss is : 1.8641433715820312 \n",
      "Epoch:8, loss is : 0.002077673329040408 \n",
      "Epoch:9, loss is : 0.0026132469065487385 \n",
      "Epoch:10, loss is : 2.6747011361294426e-06 \n",
      "Epoch:11, loss is : 5.215405707303944e-08 \n",
      "Epoch:12, loss is : 2.9802318834981634e-08 \n",
      "Epoch:13, loss is : 1.2895624422526453e-05 \n",
      "Epoch:14, loss is : 0.0002303507790202275 \n",
      "Epoch:15, loss is : 1.1026811534975423e-06 \n",
      "Epoch:16, loss is : 0.0 \n",
      "Epoch:17, loss is : 2.098480035783723e-05 \n",
      "Epoch:18, loss is : 0.032306089997291565 \n",
      "Epoch:19, loss is : 1.2277352652745321e-05 \n"
     ]
    }
   ],
   "source": [
    "# #  training flow \n",
    "# if __name__==\"__main__\":\n",
    "#     for epoc in range(epoch):\n",
    "#         for batch in train_dataloader:\n",
    "#             X,y=batch\n",
    "#             X,y=X.to(device),y.to(device)\n",
    "#             y_hat=clf(X)\n",
    "#             loss=loss_func(y_hat,y)\n",
    "\n",
    "\n",
    "#             # back propagation\n",
    "#             opt.zero_grad()\n",
    "#             loss.backward()\n",
    "#             opt.step()\n",
    "\n",
    "      \n",
    "#         print(f\"Epoch:{epoc}, loss is : {loss} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d1a5ad07-618d-46f0-89ca-de6ee3bbe039",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_state.pt','wb') as f:\n",
    "    save(clf.state_dict(),f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "58f72736-98a7-4c0c-acdd-45a45fd16a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_state.pt', 'rb') as f:\n",
    "    clf.load_state_dict(load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c8c12258-e716-42e7-a85c-d8f013c8641d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassifier(\n",
       "  (model): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (5): ReLU()\n",
       "    (6): Flatten(start_dim=1, end_dim=-1)\n",
       "    (7): Linear(in_features=86528, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare image for model\n",
    "image = image.unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "\n",
    "# Initialize the model\n",
    "clf = ImageClassifier().to(device)\n",
    "\n",
    "# Load the state dictionary\n",
    "clf.load_state_dict(torch.load('model_state.pt'))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "clf.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "7bb7552e-2000-4318-be37-f4a342364237",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.8980392..1.0].\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Select an image for prediction\n",
    "image, label = test_dataset[0]\n",
    "plt.imshow(image.permute(1, 2, 0))  # Display image\n",
    "plt.title(f'Label: {label}')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "df91655e-83ab-4e44-8a52-bec219df4f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: 3\n"
     ]
    }
   ],
   "source": [
    "# Make prediction\n",
    "with torch.no_grad():\n",
    "    output = clf(image)\n",
    "    predicted_label = torch.argmax(output)\n",
    "\n",
    "print(f'Predicted label: {predicted_label.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffdeda8-1cad-4682-9bc6-d91d3f3516c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
